{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc09042",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from matplotlib import axes\n",
    "import os \n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import sys, os\n",
    "sys.path.append('/cluster/home/kamara/Explain')\n",
    "from clutils.nbutils import *\n",
    "from clutils.nbutils.params import get_param_ranges\n",
    "os.getcwd()\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194aac31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(logdir, dataset):\n",
    "    logs = {}\n",
    "    train_scores, test_scores = {}, {}\n",
    "    for filename in os.listdir(logdir):\n",
    "        if filename.endswith(\".stdout\") and filename.startswith(\"_\"):\n",
    "            if dataset in filename:\n",
    "                _, logs[filename.replace(\".stdout\", \"\")] = parseLogs(join(logdir, filename), kw='__logs')\n",
    "                _, train_scores[filename.replace(\".stdout\", \"\")] = parseLogs(join(logdir, filename), kw='__gnn_train_scores')\n",
    "                _, test_scores[filename.replace(\".stdout\", \"\")] = parseLogs(join(logdir, filename), kw='__gnn_test_scores')\n",
    "    return(logs, train_scores, test_scores)\n",
    "\n",
    "def get_df_results(logs, ranges, name, metrics, selection = 'last'):\n",
    "    dicts = []\n",
    "    for params_set in enumerateParams(ranges):\n",
    "        key = name.format(**params_set)\n",
    "        # if key in logs and len(logs[key]) >= 1 and metric in logs[key]:\n",
    "        any_metric = (key in logs) and (len(logs[key]) >= 1) and any([metric in logs[key] for metric in metrics])\n",
    "        if any_metric:\n",
    "            if selection == 'all':\n",
    "                for index, row in logs[key].iterrows():\n",
    "                    metrics_dict = {\n",
    "                        metric: select_value(row.to_frame().T, metric, 'last') if key in logs and len(logs[key]) >= 1 and metric in logs[key] else -1\n",
    "                        for metric in metrics\n",
    "                    }\n",
    "                    dicts.append(dictmerge(params_set, metrics_dict))\n",
    "                    \n",
    "            else:\n",
    "                metrics_dict = {\n",
    "                    metric: select_value(logs[key], metric, selection) if key in logs and len(logs[key]) >= 1 and metric in logs[key] else -1\n",
    "                    for metric in metrics\n",
    "                }\n",
    "                dicts.append(dictmerge(params_set, metrics_dict))\n",
    "\n",
    "    df = pd.DataFrame(dicts)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188f5955",
   "metadata": {},
   "source": [
    "# Gridsearch GNN for FacebookPagePage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d847434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = f'/cluster/home/kamara/Explain/checkpoints/node_classification/facebook/gridsearch_facebook_lr/logs'\n",
    "jsonpath = f'/cluster/home/kamara/Explain/checkpoints/node_classification/facebook/gridsearch_facebook_lr/sweep.json'\n",
    "logs, train_scores, test_scores = get_info(logdir)\n",
    "\n",
    "print(list(logs.keys())[1])\n",
    "name = '{none}_explainer_name={explainer_name}_sparsity={sparsity}_dataset={dataset}_hard_mask={hard_mask}'\n",
    "\n",
    "ranges = get_param_ranges(jsonpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5448de44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize = (12,6), sharex=True, sharey=True)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plot_lines = []\n",
    "for params_set in enumerateParams(ranges):\n",
    "    key = name.format(**params_set)\n",
    "    v = logs[key]\n",
    "    if v.empty == False:\n",
    "        val_losses = v[\"val_err\"]\n",
    "        train_losses = v[\"train_err\"]\n",
    "        nepochs = len(val_losses)*10\n",
    "        lr = params_set['lr']\n",
    "        i = ranges['lr'].index(lr)\n",
    "        \n",
    "        l1, = ax.plot(train_losses, label=lr, c = palette[i])\n",
    "        l2, = ax.plot(val_losses, c = palette[i], ls ='dashdot')\n",
    "        plot_lines += [l1]\n",
    "        \n",
    "# set labels\n",
    "plt.setp(ax, xlabel=\"epoch\")\n",
    "plt.setp(ax, ylabel=\"avg squared error\")\n",
    "\n",
    "ax.set_title(\"Gridsearch on learning rate for FacebookPagePage\")\n",
    "\n",
    "legend1 = plt.legend(plot_lines, ranges['lr'], bbox_to_anchor=(1.15, 0.7), loc = 'center', title='learning rates')\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "legend2 = plt.legend([l1, l2], ['train', 'val'], title='error type', bbox_to_anchor=(1.15, 0.2), loc = 'center')\n",
    "plt.gca().add_artist(legend2)\n",
    "\n",
    "plt.grid(True)\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db456c75",
   "metadata": {},
   "source": [
    "# Gridsearch GNN for Cora, CiteSeer and PubMed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f0a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = f'/cluster/home/kamara/Explain/checkpoints/node_classification/planetoid/test/logs'\n",
    "jsonpath = f'/cluster/home/kamara/Explain/checkpoints/node_classification/planetoid/test/sweep.json'\n",
    "logs, train_scores, test_scores = get_info(logdir)\n",
    "\n",
    "print(list(logs.keys())[1])\n",
    "name = '{none}_explainer_name={explainer_name}_sparsity={sparsity}_dataset={dataset}_hard_mask={hard_mask}'\n",
    "\n",
    "ranges_all = get_param_ranges(jsonpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe98719",
   "metadata": {},
   "source": [
    "## Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c998a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix weight decay, dropout\n",
    "df = logs[(logs.weight_decay==0.0)&(logs.dropout==0.0)]\n",
    "ranges = ranges_all.copy()\n",
    "ranges['weight_decay'] == 0\n",
    "ranges['dropout'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b3b959",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette(\"Paired\", 25)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize = (12,6), sharex=True, sharey=True)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plot_lines = []\n",
    "for params_set in enumerateParams(ranges):\n",
    "    key = name.format(**params_set)\n",
    "    v = logs[key]\n",
    "    if v.empty == False:\n",
    "        val_losses = v[\"val_err\"]\n",
    "        train_losses = v[\"train_err\"]\n",
    "        nepochs = len(val_losses)*10\n",
    "        dataset = params_set['dataset']\n",
    "        lr = params_set['lr']\n",
    "        i = ranges['lr'].index(lr)\n",
    "\n",
    "        if dataset=='Cora':\n",
    "            l1, = axs[0,0].plot(train_losses, label=lr, c = palette[i])\n",
    "            l2, = axs[0,0].plot(val_losses, c = palette[i], ls ='dashdot')\n",
    "\n",
    "        elif optimizer=='CiteSeer':\n",
    "            l1, = axs[0,1].plot(train_losses, label=lr, c = palette[i])\n",
    "            l2, = axs[0,1].plot(val_losses, c = palette[i], ls ='dashdot')\n",
    "\n",
    "        elif optimizer=='PubMed':\n",
    "            l1, = axs[1,0].plot(train_losses, label=lr, c = palette[i])\n",
    "            l2, = axs[1,0].plot(val_losses, c = palette[i], ls ='dashdot')\n",
    "\n",
    "        else:\n",
    "            print('Not an neural net')\n",
    "\n",
    "\n",
    "axs[0,0].set_title(f\"Cora\")\n",
    "axs[0,1].set_title(f\"CiteSeer\")\n",
    "axs[1,0].set_title(f\"PubMed\")\n",
    "\n",
    "# set labels\n",
    "plt.setp(axs, xlabel=\"epoch\")\n",
    "plt.setp(axs, ylabel=\"avg squared error\")\n",
    "\n",
    "legend1 = plt.legend(plot_lines, ranges['lr'], bbox_to_anchor=(1.15, 1.45), loc = 'center', title='learning rate')\n",
    "plt.gca().add_artist(legend1)\n",
    "\n",
    "legend2 = plt.legend([l1, l2], ['train', 'val'], title='error type', bbox_to_anchor=(1.15, 0.2), loc = 'center')\n",
    "plt.gca().add_artist(legend2)\n",
    "\n",
    "fig.suptitle('Gridsearch on learning rate', fontsize=16)\n",
    "plt.grid(True)\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
