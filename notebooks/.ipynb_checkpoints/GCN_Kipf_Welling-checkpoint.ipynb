{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2608d488",
   "metadata": {},
   "source": [
    "# Pytorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "db824b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.  0. ]\n",
      " [0.  0.  0.2]\n",
      " [0.  0.  0. ]]\n",
      "  (0, 0)\t1.0\n",
      "  (1, 1)\t0.8333333\n",
      "  (2, 1)\t0.16666667\n",
      "  (1, 2)\t0.16666667\n",
      "  (2, 2)\t0.8333333\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import from_scipy_sparse_matrix, k_hop_subgraph, to_scipy_sparse_matrix\n",
    "\n",
    "a = [[0, 0.2, 0, 0], [0, 0, 0, 0], [0.1, 0.2, 0, 0], [0, 0, 0, 0]]\n",
    "edge_index = torch.LongTensor([[0, 0], [1, 2]])\n",
    "edge_weight = torch.FloatTensor([0.1, 0.2])\n",
    "\n",
    "adj = sp.coo_matrix(\n",
    "        (edge_weight, (edge_index[:, 0], edge_index[:, 1])), shape=(3, 3), dtype=np.float32\n",
    "    )\n",
    "print(adj.toarray())\n",
    "# build symmetric adjacency matrix\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "adj = adj.tocoo().astype(np.float32)\n",
    "print(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "10ff6527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.from_numpy(\n",
    "        np.vstack((adj.row, adj.col)).astype(np.int64))\n",
    "indices.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6a78b671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "abe5591a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.8333, 0.1667, 0.1667, 0.8333])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = torch.from_numpy(adj.data)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "80ff6a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1000, 0.2000])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ab048464",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = torch.Size(adj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cc5bf26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 1, 2, 1, 2],\n",
       "                       [0, 1, 1, 2, 2]]),\n",
       "       values=tensor([1.0000, 0.8333, 0.1667, 0.1667, 0.8333]),\n",
       "       size=(3, 3), nnz=5, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "59b2cfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 0],\n",
       "                       [1, 2]]),\n",
       "       values=tensor([0.1000, 0.2000]),\n",
       "       size=(3, 3), nnz=2, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sparse.FloatTensor(edge_index, edge_weight, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a021ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.from_numpy(\n",
    "        np.vstack((adj.row, adj.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(adj.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7c0650b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new() received an invalid combination of arguments - got (Tensor, Tensor, shape=torch.Size), but expected one of:\n * (*, torch.device device)\n * (Tensor indices, Tensor values, *, torch.device device)\n * (Tensor indices, Tensor values, tuple of ints size, *, torch.device device)\n * (tuple of ints size, *, torch.device device)\n      didn't match because some of the keywords were incorrect: shape\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24791/1176992988.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (Tensor, Tensor, shape=torch.Size), but expected one of:\n * (*, torch.device device)\n * (Tensor indices, Tensor values, *, torch.device device)\n * (Tensor indices, Tensor values, tuple of ints size, *, torch.device device)\n * (tuple of ints size, *, torch.device device)\n      didn't match because some of the keywords were incorrect: shape\n"
     ]
    }
   ],
   "source": [
    "torch.spmm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c9e06666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('/cluster/home/kamara/Explain/code')\n",
    "\n",
    "from utils.gen_utils import from_adj_to_edge_index, from_edge_index_to_adj, init_weights\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "01b31b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, edge_index, edge_weight):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        shape = torch.Size((len(input), len(input)))\n",
    "        adj = torch.sparse.FloatTensor(edge_index, edge_weight, shape)\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "    \n",
    "    \n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
    "        self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, num_node_features, hidden_dim, num_classes, dropout, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_node_features, self.num_classes, self.num_layers, self.hidden_dim, self.dropout = (\n",
    "            num_node_features,\n",
    "            num_classes,\n",
    "            num_layers,\n",
    "            hidden_dim,\n",
    "            dropout,\n",
    "        )\n",
    "        self.layers = nn.ModuleList()\n",
    "        current_dim = self.num_node_features\n",
    "        for l in range(self.num_layers - 1):\n",
    "            self.layers.append(GraphConvolution(current_dim, hidden_dim))\n",
    "            current_dim = hidden_dim\n",
    "        self.layers.append(GraphConvolution(current_dim, self.num_classes))\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x, edge_index, edge_weight)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.layers[-1](x, edge_index, edge_weight)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c72784dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset_str):\n",
    "    data_filename = \"/cluster/home/kamara/Explain/data/{}/processed/data.pt\".format(PLANETOIDS[dataset_str])\n",
    "    data, _ = torch.load(data_filename)\n",
    "    data = preprocess_planetoid(data)\n",
    "    \n",
    "    idx_train = data.train_mask\n",
    "    idx_val = data.val_mask\n",
    "    idx_test = data.test_mask\n",
    "    \n",
    "    return data, data.adj, data.x, data.y, idx_train, idx_val, idx_test\n",
    "    \n",
    "\n",
    "def preprocess_planetoid(data):\n",
    "    edges = data.edge_index.T\n",
    "    adj = sp.coo_matrix(\n",
    "        (np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(data.num_nodes, data.num_nodes), dtype=np.float32\n",
    "    )\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    #data.x = torch.FloatTensor(np.array(data.x))\n",
    "    labels = torch.LongTensor(data.y)\n",
    "\n",
    "    sparse_mx = adj.tocoo().astype(np.float32)\n",
    "    data.edge_index = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    data.edge_weight = torch.from_numpy(sparse_mx.data)\n",
    "    data.adj = sparse_mx_to_torch_sparse_tensor(sparse_mx)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.0\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e8cd3078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.edge_weight tensor([0.1667, 0.0270, 0.0278,  ..., 0.5000, 0.1250, 0.5000])\n",
      "Epoch: 0001 loss_train: 1.1533 acc_train: 0.3500 loss_val: 1.2180 acc_val: 0.1980 time: 0.0300s\n",
      "Epoch: 0002 loss_train: 1.1442 acc_train: 0.3500 loss_val: 1.1955 acc_val: 0.3240 time: 0.0299s\n",
      "Epoch: 0003 loss_train: 1.1319 acc_train: 0.4000 loss_val: 1.1746 acc_val: 0.4860 time: 0.0283s\n",
      "Epoch: 0004 loss_train: 1.1218 acc_train: 0.4667 loss_val: 1.1547 acc_val: 0.5220 time: 0.0282s\n",
      "Epoch: 0005 loss_train: 1.1041 acc_train: 0.5500 loss_val: 1.1350 acc_val: 0.5340 time: 0.0287s\n",
      "Epoch: 0006 loss_train: 1.0722 acc_train: 0.5833 loss_val: 1.1149 acc_val: 0.5220 time: 0.0281s\n",
      "Epoch: 0007 loss_train: 1.0488 acc_train: 0.5833 loss_val: 1.0951 acc_val: 0.5280 time: 0.0280s\n",
      "Epoch: 0008 loss_train: 1.0358 acc_train: 0.6000 loss_val: 1.0763 acc_val: 0.5260 time: 0.0306s\n",
      "Epoch: 0009 loss_train: 1.0275 acc_train: 0.5833 loss_val: 1.0587 acc_val: 0.5240 time: 0.0280s\n",
      "Epoch: 0010 loss_train: 1.0057 acc_train: 0.5500 loss_val: 1.0417 acc_val: 0.5280 time: 0.0279s\n",
      "Epoch: 0011 loss_train: 0.9950 acc_train: 0.6167 loss_val: 1.0256 acc_val: 0.5260 time: 0.0280s\n",
      "Epoch: 0012 loss_train: 0.9601 acc_train: 0.6667 loss_val: 1.0094 acc_val: 0.5320 time: 0.0290s\n",
      "Epoch: 0013 loss_train: 0.9343 acc_train: 0.6667 loss_val: 0.9927 acc_val: 0.5420 time: 0.0279s\n",
      "Epoch: 0014 loss_train: 0.9439 acc_train: 0.6667 loss_val: 0.9761 acc_val: 0.5760 time: 0.0279s\n",
      "Epoch: 0015 loss_train: 0.8996 acc_train: 0.7500 loss_val: 0.9594 acc_val: 0.6160 time: 0.0280s\n",
      "Epoch: 0016 loss_train: 0.8865 acc_train: 0.7167 loss_val: 0.9426 acc_val: 0.6640 time: 0.0306s\n",
      "Epoch: 0017 loss_train: 0.8743 acc_train: 0.7167 loss_val: 0.9263 acc_val: 0.7020 time: 0.0279s\n",
      "Epoch: 0018 loss_train: 0.8340 acc_train: 0.7500 loss_val: 0.9110 acc_val: 0.7160 time: 0.0279s\n",
      "Epoch: 0019 loss_train: 0.8217 acc_train: 0.8333 loss_val: 0.8962 acc_val: 0.7300 time: 0.0282s\n",
      "Epoch: 0020 loss_train: 0.8163 acc_train: 0.7667 loss_val: 0.8820 acc_val: 0.7400 time: 0.0287s\n",
      "Epoch: 0021 loss_train: 0.8119 acc_train: 0.7833 loss_val: 0.8682 acc_val: 0.7440 time: 0.0282s\n",
      "Epoch: 0022 loss_train: 0.7502 acc_train: 0.8667 loss_val: 0.8547 acc_val: 0.7440 time: 0.0281s\n",
      "Epoch: 0023 loss_train: 0.7668 acc_train: 0.8167 loss_val: 0.8410 acc_val: 0.7440 time: 0.0281s\n",
      "Epoch: 0024 loss_train: 0.7186 acc_train: 0.8333 loss_val: 0.8279 acc_val: 0.7480 time: 0.0282s\n",
      "Epoch: 0025 loss_train: 0.6961 acc_train: 0.9000 loss_val: 0.8140 acc_val: 0.7480 time: 0.0333s\n",
      "Epoch: 0026 loss_train: 0.7121 acc_train: 0.8000 loss_val: 0.7996 acc_val: 0.7560 time: 0.0284s\n",
      "Epoch: 0027 loss_train: 0.6182 acc_train: 0.9500 loss_val: 0.7855 acc_val: 0.7560 time: 0.0286s\n",
      "Epoch: 0028 loss_train: 0.6163 acc_train: 0.8667 loss_val: 0.7722 acc_val: 0.7580 time: 0.0283s\n",
      "Epoch: 0029 loss_train: 0.6503 acc_train: 0.8333 loss_val: 0.7593 acc_val: 0.7600 time: 0.0281s\n",
      "Epoch: 0030 loss_train: 0.6229 acc_train: 0.9167 loss_val: 0.7473 acc_val: 0.7600 time: 0.0280s\n",
      "Epoch: 0031 loss_train: 0.5892 acc_train: 0.8667 loss_val: 0.7358 acc_val: 0.7600 time: 0.0281s\n",
      "Epoch: 0032 loss_train: 0.5597 acc_train: 0.8833 loss_val: 0.7254 acc_val: 0.7580 time: 0.0293s\n",
      "Epoch: 0033 loss_train: 0.5354 acc_train: 0.9167 loss_val: 0.7152 acc_val: 0.7620 time: 0.0287s\n",
      "Epoch: 0034 loss_train: 0.5324 acc_train: 0.9333 loss_val: 0.7053 acc_val: 0.7660 time: 0.0279s\n",
      "Epoch: 0035 loss_train: 0.5113 acc_train: 0.8833 loss_val: 0.6960 acc_val: 0.7660 time: 0.0281s\n",
      "Epoch: 0036 loss_train: 0.5113 acc_train: 0.9333 loss_val: 0.6867 acc_val: 0.7680 time: 0.0281s\n",
      "Epoch: 0037 loss_train: 0.5047 acc_train: 0.8667 loss_val: 0.6781 acc_val: 0.7660 time: 0.0282s\n",
      "Epoch: 0038 loss_train: 0.4877 acc_train: 0.9167 loss_val: 0.6701 acc_val: 0.7680 time: 0.0280s\n",
      "Epoch: 0039 loss_train: 0.4422 acc_train: 0.8833 loss_val: 0.6627 acc_val: 0.7680 time: 0.0282s\n",
      "Epoch: 0040 loss_train: 0.4206 acc_train: 0.9167 loss_val: 0.6558 acc_val: 0.7680 time: 0.0309s\n",
      "Epoch: 0041 loss_train: 0.4269 acc_train: 0.9000 loss_val: 0.6491 acc_val: 0.7700 time: 0.0281s\n",
      "Epoch: 0042 loss_train: 0.4634 acc_train: 0.8833 loss_val: 0.6429 acc_val: 0.7720 time: 0.0280s\n",
      "Epoch: 0043 loss_train: 0.4092 acc_train: 0.9333 loss_val: 0.6366 acc_val: 0.7720 time: 0.0279s\n",
      "Epoch: 0044 loss_train: 0.3838 acc_train: 0.9167 loss_val: 0.6298 acc_val: 0.7740 time: 0.0279s\n",
      "Epoch: 0045 loss_train: 0.4129 acc_train: 0.9333 loss_val: 0.6233 acc_val: 0.7740 time: 0.0320s\n",
      "Epoch: 0046 loss_train: 0.3734 acc_train: 0.8667 loss_val: 0.6173 acc_val: 0.7780 time: 0.0280s\n",
      "Epoch: 0047 loss_train: 0.4059 acc_train: 0.9000 loss_val: 0.6123 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0048 loss_train: 0.3183 acc_train: 0.9500 loss_val: 0.6079 acc_val: 0.7800 time: 0.0317s\n",
      "Epoch: 0049 loss_train: 0.3452 acc_train: 0.9333 loss_val: 0.6036 acc_val: 0.7780 time: 0.0282s\n",
      "Epoch: 0050 loss_train: 0.3576 acc_train: 0.9500 loss_val: 0.5994 acc_val: 0.7780 time: 0.0283s\n",
      "Epoch: 0051 loss_train: 0.3646 acc_train: 0.9167 loss_val: 0.5958 acc_val: 0.7780 time: 0.0281s\n",
      "Epoch: 0052 loss_train: 0.3325 acc_train: 0.9333 loss_val: 0.5926 acc_val: 0.7800 time: 0.0281s\n",
      "Epoch: 0053 loss_train: 0.2832 acc_train: 0.9333 loss_val: 0.5901 acc_val: 0.7820 time: 0.0282s\n",
      "Epoch: 0054 loss_train: 0.3139 acc_train: 0.9500 loss_val: 0.5881 acc_val: 0.7840 time: 0.0279s\n",
      "Epoch: 0055 loss_train: 0.2994 acc_train: 0.9500 loss_val: 0.5873 acc_val: 0.7820 time: 0.0281s\n",
      "Epoch: 0056 loss_train: 0.3041 acc_train: 0.9000 loss_val: 0.5865 acc_val: 0.7840 time: 0.0297s\n",
      "Epoch: 0057 loss_train: 0.2888 acc_train: 0.9333 loss_val: 0.5860 acc_val: 0.7800 time: 0.0283s\n",
      "Epoch: 0058 loss_train: 0.2682 acc_train: 0.9500 loss_val: 0.5845 acc_val: 0.7760 time: 0.0281s\n",
      "Epoch: 0059 loss_train: 0.2589 acc_train: 0.9333 loss_val: 0.5823 acc_val: 0.7780 time: 0.0282s\n",
      "Epoch: 0060 loss_train: 0.2408 acc_train: 0.9500 loss_val: 0.5803 acc_val: 0.7820 time: 0.0282s\n",
      "Epoch: 0061 loss_train: 0.2522 acc_train: 0.9667 loss_val: 0.5766 acc_val: 0.7820 time: 0.0294s\n",
      "Epoch: 0062 loss_train: 0.2392 acc_train: 0.9667 loss_val: 0.5723 acc_val: 0.7720 time: 0.0285s\n",
      "Epoch: 0063 loss_train: 0.2115 acc_train: 0.9500 loss_val: 0.5686 acc_val: 0.7680 time: 0.0282s\n",
      "Epoch: 0064 loss_train: 0.2285 acc_train: 0.9500 loss_val: 0.5647 acc_val: 0.7740 time: 0.0292s\n",
      "Epoch: 0065 loss_train: 0.2102 acc_train: 0.9667 loss_val: 0.5609 acc_val: 0.7760 time: 0.0289s\n",
      "Epoch: 0066 loss_train: 0.1899 acc_train: 0.9667 loss_val: 0.5572 acc_val: 0.7800 time: 0.0306s\n",
      "Epoch: 0067 loss_train: 0.1951 acc_train: 0.9667 loss_val: 0.5545 acc_val: 0.7780 time: 0.0281s\n",
      "Epoch: 0068 loss_train: 0.1757 acc_train: 0.9667 loss_val: 0.5530 acc_val: 0.7820 time: 0.0281s\n",
      "Epoch: 0069 loss_train: 0.1931 acc_train: 0.9667 loss_val: 0.5528 acc_val: 0.7800 time: 0.0283s\n",
      "Epoch: 0070 loss_train: 0.2099 acc_train: 0.9667 loss_val: 0.5538 acc_val: 0.7780 time: 0.0295s\n",
      "Epoch: 0071 loss_train: 0.1745 acc_train: 0.9833 loss_val: 0.5557 acc_val: 0.7820 time: 0.0285s\n",
      "Epoch: 0072 loss_train: 0.1545 acc_train: 0.9833 loss_val: 0.5572 acc_val: 0.7840 time: 0.0307s\n",
      "Epoch: 0073 loss_train: 0.1808 acc_train: 0.9833 loss_val: 0.5589 acc_val: 0.7840 time: 0.0282s\n",
      "Epoch: 0074 loss_train: 0.1536 acc_train: 0.9833 loss_val: 0.5605 acc_val: 0.7800 time: 0.0294s\n",
      "Epoch: 0075 loss_train: 0.1401 acc_train: 1.0000 loss_val: 0.5619 acc_val: 0.7800 time: 0.0283s\n",
      "Epoch: 0076 loss_train: 0.1713 acc_train: 1.0000 loss_val: 0.5630 acc_val: 0.7760 time: 0.0281s\n",
      "Epoch: 0077 loss_train: 0.1597 acc_train: 0.9667 loss_val: 0.5632 acc_val: 0.7760 time: 0.0282s\n",
      "Epoch: 0078 loss_train: 0.1635 acc_train: 0.9667 loss_val: 0.5626 acc_val: 0.7760 time: 0.0281s\n",
      "Epoch: 0079 loss_train: 0.1398 acc_train: 0.9833 loss_val: 0.5606 acc_val: 0.7780 time: 0.0330s\n",
      "Epoch: 0080 loss_train: 0.1491 acc_train: 0.9833 loss_val: 0.5579 acc_val: 0.7820 time: 0.0292s\n",
      "Epoch: 0081 loss_train: 0.1387 acc_train: 1.0000 loss_val: 0.5567 acc_val: 0.7840 time: 0.0282s\n",
      "Epoch: 0082 loss_train: 0.1409 acc_train: 1.0000 loss_val: 0.5548 acc_val: 0.7860 time: 0.0282s\n",
      "Epoch: 0083 loss_train: 0.1624 acc_train: 0.9833 loss_val: 0.5540 acc_val: 0.7860 time: 0.0282s\n",
      "Epoch: 0084 loss_train: 0.1551 acc_train: 0.9833 loss_val: 0.5535 acc_val: 0.7860 time: 0.0282s\n",
      "Epoch: 0085 loss_train: 0.1523 acc_train: 1.0000 loss_val: 0.5530 acc_val: 0.7860 time: 0.0280s\n",
      "Epoch: 0086 loss_train: 0.1438 acc_train: 0.9833 loss_val: 0.5518 acc_val: 0.7900 time: 0.0281s\n",
      "Epoch: 0087 loss_train: 0.1510 acc_train: 1.0000 loss_val: 0.5504 acc_val: 0.7840 time: 0.0280s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0088 loss_train: 0.1668 acc_train: 0.9667 loss_val: 0.5496 acc_val: 0.7840 time: 0.0297s\n",
      "Epoch: 0089 loss_train: 0.1401 acc_train: 1.0000 loss_val: 0.5491 acc_val: 0.7840 time: 0.0281s\n",
      "Epoch: 0090 loss_train: 0.1463 acc_train: 1.0000 loss_val: 0.5502 acc_val: 0.7840 time: 0.0285s\n",
      "Epoch: 0091 loss_train: 0.1444 acc_train: 0.9667 loss_val: 0.5511 acc_val: 0.7860 time: 0.0308s\n",
      "Epoch: 0092 loss_train: 0.1257 acc_train: 1.0000 loss_val: 0.5516 acc_val: 0.7860 time: 0.0280s\n",
      "Epoch: 0093 loss_train: 0.1691 acc_train: 0.9667 loss_val: 0.5531 acc_val: 0.7880 time: 0.0281s\n",
      "Epoch: 0094 loss_train: 0.1244 acc_train: 0.9833 loss_val: 0.5551 acc_val: 0.7880 time: 0.0296s\n",
      "Epoch: 0095 loss_train: 0.1512 acc_train: 0.9667 loss_val: 0.5564 acc_val: 0.7880 time: 0.0286s\n",
      "Epoch: 0096 loss_train: 0.1258 acc_train: 1.0000 loss_val: 0.5573 acc_val: 0.7880 time: 0.0294s\n",
      "Epoch: 0097 loss_train: 0.1466 acc_train: 1.0000 loss_val: 0.5579 acc_val: 0.7900 time: 0.0281s\n",
      "Epoch: 0098 loss_train: 0.1670 acc_train: 0.9833 loss_val: 0.5563 acc_val: 0.7940 time: 0.0281s\n",
      "Epoch: 0099 loss_train: 0.1271 acc_train: 1.0000 loss_val: 0.5546 acc_val: 0.7940 time: 0.0281s\n",
      "Epoch: 0100 loss_train: 0.1292 acc_train: 0.9833 loss_val: 0.5530 acc_val: 0.7940 time: 0.0279s\n",
      "Epoch: 0101 loss_train: 0.1094 acc_train: 1.0000 loss_val: 0.5519 acc_val: 0.7900 time: 0.0281s\n",
      "Epoch: 0102 loss_train: 0.1118 acc_train: 1.0000 loss_val: 0.5507 acc_val: 0.7880 time: 0.0282s\n",
      "Epoch: 0103 loss_train: 0.1133 acc_train: 1.0000 loss_val: 0.5494 acc_val: 0.7860 time: 0.0295s\n",
      "Epoch: 0104 loss_train: 0.1073 acc_train: 1.0000 loss_val: 0.5490 acc_val: 0.7900 time: 0.0313s\n",
      "Epoch: 0105 loss_train: 0.1137 acc_train: 1.0000 loss_val: 0.5495 acc_val: 0.7900 time: 0.0282s\n",
      "Epoch: 0106 loss_train: 0.1443 acc_train: 0.9833 loss_val: 0.5506 acc_val: 0.7840 time: 0.0282s\n",
      "Epoch: 0107 loss_train: 0.1074 acc_train: 0.9833 loss_val: 0.5523 acc_val: 0.7860 time: 0.0293s\n",
      "Epoch: 0108 loss_train: 0.1296 acc_train: 0.9833 loss_val: 0.5554 acc_val: 0.7840 time: 0.0287s\n",
      "Epoch: 0109 loss_train: 0.1035 acc_train: 1.0000 loss_val: 0.5584 acc_val: 0.7840 time: 0.0281s\n",
      "Epoch: 0110 loss_train: 0.1274 acc_train: 0.9833 loss_val: 0.5595 acc_val: 0.7840 time: 0.0281s\n",
      "Epoch: 0111 loss_train: 0.0889 acc_train: 1.0000 loss_val: 0.5620 acc_val: 0.7840 time: 0.0283s\n",
      "Epoch: 0112 loss_train: 0.1050 acc_train: 1.0000 loss_val: 0.5642 acc_val: 0.7900 time: 0.0604s\n",
      "Epoch: 0113 loss_train: 0.1171 acc_train: 1.0000 loss_val: 0.5654 acc_val: 0.7900 time: 0.0291s\n",
      "Epoch: 0114 loss_train: 0.1152 acc_train: 1.0000 loss_val: 0.5654 acc_val: 0.7920 time: 0.0283s\n",
      "Epoch: 0115 loss_train: 0.1299 acc_train: 0.9833 loss_val: 0.5650 acc_val: 0.7940 time: 0.0287s\n",
      "Epoch: 0116 loss_train: 0.1067 acc_train: 1.0000 loss_val: 0.5623 acc_val: 0.7880 time: 0.0326s\n",
      "Epoch: 0117 loss_train: 0.0912 acc_train: 1.0000 loss_val: 0.5599 acc_val: 0.7860 time: 0.0282s\n",
      "Epoch: 0118 loss_train: 0.1218 acc_train: 1.0000 loss_val: 0.5564 acc_val: 0.7880 time: 0.0287s\n",
      "Epoch: 0119 loss_train: 0.1314 acc_train: 0.9833 loss_val: 0.5546 acc_val: 0.7840 time: 0.0285s\n",
      "Epoch: 0120 loss_train: 0.1338 acc_train: 0.9833 loss_val: 0.5532 acc_val: 0.7860 time: 0.0298s\n",
      "Epoch: 0121 loss_train: 0.1326 acc_train: 0.9833 loss_val: 0.5537 acc_val: 0.7800 time: 0.0290s\n",
      "Epoch: 0122 loss_train: 0.1255 acc_train: 1.0000 loss_val: 0.5557 acc_val: 0.7800 time: 0.0285s\n",
      "Epoch: 0123 loss_train: 0.1294 acc_train: 1.0000 loss_val: 0.5584 acc_val: 0.7840 time: 0.0286s\n",
      "Epoch: 0124 loss_train: 0.0973 acc_train: 1.0000 loss_val: 0.5627 acc_val: 0.7840 time: 0.0288s\n",
      "Epoch: 0125 loss_train: 0.1459 acc_train: 0.9667 loss_val: 0.5674 acc_val: 0.7740 time: 0.0281s\n",
      "Epoch: 0126 loss_train: 0.0883 acc_train: 1.0000 loss_val: 0.5711 acc_val: 0.7760 time: 0.0285s\n",
      "Epoch: 0127 loss_train: 0.0844 acc_train: 1.0000 loss_val: 0.5743 acc_val: 0.7820 time: 0.0289s\n",
      "Epoch: 0128 loss_train: 0.1163 acc_train: 0.9833 loss_val: 0.5735 acc_val: 0.7840 time: 0.0309s\n",
      "Epoch: 0129 loss_train: 0.1214 acc_train: 1.0000 loss_val: 0.5715 acc_val: 0.7860 time: 0.0283s\n",
      "Epoch: 0130 loss_train: 0.0775 acc_train: 1.0000 loss_val: 0.5697 acc_val: 0.7820 time: 0.0286s\n",
      "Epoch: 0131 loss_train: 0.1081 acc_train: 1.0000 loss_val: 0.5673 acc_val: 0.7840 time: 0.0282s\n",
      "Epoch: 0132 loss_train: 0.0977 acc_train: 1.0000 loss_val: 0.5651 acc_val: 0.7860 time: 0.0285s\n",
      "Epoch: 0133 loss_train: 0.1206 acc_train: 0.9667 loss_val: 0.5636 acc_val: 0.7860 time: 0.0421s\n",
      "Epoch: 0134 loss_train: 0.1064 acc_train: 0.9833 loss_val: 0.5623 acc_val: 0.7840 time: 0.0333s\n",
      "Epoch: 0135 loss_train: 0.1151 acc_train: 0.9833 loss_val: 0.5589 acc_val: 0.7840 time: 0.0301s\n",
      "Epoch: 0136 loss_train: 0.0843 acc_train: 1.0000 loss_val: 0.5554 acc_val: 0.7840 time: 0.0360s\n",
      "Epoch: 0137 loss_train: 0.1306 acc_train: 1.0000 loss_val: 0.5525 acc_val: 0.7880 time: 0.0297s\n",
      "Epoch: 0138 loss_train: 0.1064 acc_train: 1.0000 loss_val: 0.5518 acc_val: 0.7860 time: 0.0414s\n",
      "Epoch: 0139 loss_train: 0.1058 acc_train: 0.9833 loss_val: 0.5517 acc_val: 0.7880 time: 0.0283s\n",
      "Epoch: 0140 loss_train: 0.0888 acc_train: 1.0000 loss_val: 0.5520 acc_val: 0.7880 time: 0.0394s\n",
      "Epoch: 0141 loss_train: 0.0944 acc_train: 1.0000 loss_val: 0.5534 acc_val: 0.7920 time: 0.0404s\n",
      "Epoch: 0142 loss_train: 0.1071 acc_train: 0.9833 loss_val: 0.5552 acc_val: 0.7880 time: 0.0285s\n",
      "Epoch: 0143 loss_train: 0.1112 acc_train: 1.0000 loss_val: 0.5566 acc_val: 0.7840 time: 0.0301s\n",
      "Epoch: 0144 loss_train: 0.0949 acc_train: 1.0000 loss_val: 0.5574 acc_val: 0.7820 time: 0.0280s\n",
      "Epoch: 0145 loss_train: 0.0818 acc_train: 1.0000 loss_val: 0.5592 acc_val: 0.7840 time: 0.0278s\n",
      "Epoch: 0146 loss_train: 0.0715 acc_train: 1.0000 loss_val: 0.5602 acc_val: 0.7840 time: 0.0277s\n",
      "Epoch: 0147 loss_train: 0.0724 acc_train: 1.0000 loss_val: 0.5584 acc_val: 0.7820 time: 0.0277s\n",
      "Epoch: 0148 loss_train: 0.1007 acc_train: 0.9833 loss_val: 0.5568 acc_val: 0.7880 time: 0.0278s\n",
      "Epoch: 0149 loss_train: 0.0802 acc_train: 1.0000 loss_val: 0.5554 acc_val: 0.7860 time: 0.0304s\n",
      "Epoch: 0150 loss_train: 0.0780 acc_train: 1.0000 loss_val: 0.5546 acc_val: 0.7800 time: 0.0277s\n",
      "Epoch: 0151 loss_train: 0.0798 acc_train: 1.0000 loss_val: 0.5541 acc_val: 0.7800 time: 0.0278s\n",
      "Epoch: 0152 loss_train: 0.1013 acc_train: 0.9833 loss_val: 0.5528 acc_val: 0.7860 time: 0.0277s\n",
      "Epoch: 0153 loss_train: 0.0868 acc_train: 1.0000 loss_val: 0.5552 acc_val: 0.7840 time: 0.0277s\n",
      "Epoch: 0154 loss_train: 0.0770 acc_train: 1.0000 loss_val: 0.5574 acc_val: 0.7820 time: 0.0278s\n",
      "Epoch: 0155 loss_train: 0.1122 acc_train: 1.0000 loss_val: 0.5593 acc_val: 0.7840 time: 0.0278s\n",
      "Epoch: 0156 loss_train: 0.0959 acc_train: 1.0000 loss_val: 0.5596 acc_val: 0.7820 time: 0.0275s\n",
      "Epoch: 0157 loss_train: 0.0919 acc_train: 1.0000 loss_val: 0.5610 acc_val: 0.7840 time: 0.0294s\n",
      "Epoch: 0158 loss_train: 0.0945 acc_train: 1.0000 loss_val: 0.5614 acc_val: 0.7840 time: 0.0278s\n",
      "Epoch: 0159 loss_train: 0.0661 acc_train: 1.0000 loss_val: 0.5608 acc_val: 0.7840 time: 0.0277s\n",
      "Epoch: 0160 loss_train: 0.0691 acc_train: 1.0000 loss_val: 0.5594 acc_val: 0.7820 time: 0.0278s\n",
      "Epoch: 0161 loss_train: 0.0838 acc_train: 0.9833 loss_val: 0.5580 acc_val: 0.7840 time: 0.0278s\n",
      "Epoch: 0162 loss_train: 0.0848 acc_train: 1.0000 loss_val: 0.5566 acc_val: 0.7860 time: 0.0278s\n",
      "Epoch: 0163 loss_train: 0.0763 acc_train: 1.0000 loss_val: 0.5565 acc_val: 0.7840 time: 0.0277s\n",
      "Epoch: 0164 loss_train: 0.0825 acc_train: 1.0000 loss_val: 0.5573 acc_val: 0.7840 time: 0.0278s\n",
      "Epoch: 0165 loss_train: 0.0699 acc_train: 1.0000 loss_val: 0.5576 acc_val: 0.7860 time: 0.0291s\n",
      "Epoch: 0166 loss_train: 0.0791 acc_train: 1.0000 loss_val: 0.5586 acc_val: 0.7880 time: 0.0278s\n",
      "Epoch: 0167 loss_train: 0.0995 acc_train: 0.9833 loss_val: 0.5605 acc_val: 0.7900 time: 0.0275s\n",
      "Epoch: 0168 loss_train: 0.0774 acc_train: 1.0000 loss_val: 0.5642 acc_val: 0.7900 time: 0.0278s\n",
      "Epoch: 0169 loss_train: 0.0752 acc_train: 0.9833 loss_val: 0.5687 acc_val: 0.7880 time: 0.0277s\n",
      "Epoch: 0170 loss_train: 0.0731 acc_train: 1.0000 loss_val: 0.5723 acc_val: 0.7860 time: 0.0277s\n",
      "Epoch: 0171 loss_train: 0.0729 acc_train: 1.0000 loss_val: 0.5773 acc_val: 0.7740 time: 0.0277s\n",
      "Epoch: 0172 loss_train: 0.0804 acc_train: 1.0000 loss_val: 0.5795 acc_val: 0.7740 time: 0.0277s\n",
      "Epoch: 0173 loss_train: 0.0592 acc_train: 1.0000 loss_val: 0.5807 acc_val: 0.7720 time: 0.0296s\n",
      "Epoch: 0174 loss_train: 0.0804 acc_train: 1.0000 loss_val: 0.5792 acc_val: 0.7740 time: 0.0277s\n",
      "Epoch: 0175 loss_train: 0.0790 acc_train: 0.9833 loss_val: 0.5757 acc_val: 0.7780 time: 0.0278s\n",
      "Epoch: 0176 loss_train: 0.0686 acc_train: 1.0000 loss_val: 0.5694 acc_val: 0.7780 time: 0.0279s\n",
      "Epoch: 0177 loss_train: 0.0691 acc_train: 1.0000 loss_val: 0.5659 acc_val: 0.7840 time: 0.0278s\n",
      "Epoch: 0178 loss_train: 0.0762 acc_train: 1.0000 loss_val: 0.5656 acc_val: 0.7780 time: 0.0278s\n",
      "Epoch: 0179 loss_train: 0.0939 acc_train: 0.9833 loss_val: 0.5662 acc_val: 0.7760 time: 0.0277s\n",
      "Epoch: 0180 loss_train: 0.0735 acc_train: 1.0000 loss_val: 0.5675 acc_val: 0.7700 time: 0.0282s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0181 loss_train: 0.0733 acc_train: 1.0000 loss_val: 0.5682 acc_val: 0.7740 time: 0.0292s\n",
      "Epoch: 0182 loss_train: 0.0660 acc_train: 1.0000 loss_val: 0.5683 acc_val: 0.7740 time: 0.0277s\n",
      "Epoch: 0183 loss_train: 0.0814 acc_train: 1.0000 loss_val: 0.5680 acc_val: 0.7760 time: 0.0277s\n",
      "Epoch: 0184 loss_train: 0.0787 acc_train: 0.9833 loss_val: 0.5676 acc_val: 0.7800 time: 0.0277s\n",
      "Epoch: 0185 loss_train: 0.0915 acc_train: 1.0000 loss_val: 0.5682 acc_val: 0.7780 time: 0.0278s\n",
      "Epoch: 0186 loss_train: 0.0786 acc_train: 1.0000 loss_val: 0.5681 acc_val: 0.7780 time: 0.0277s\n",
      "Epoch: 0187 loss_train: 0.0909 acc_train: 0.9833 loss_val: 0.5663 acc_val: 0.7800 time: 0.0276s\n",
      "Epoch: 0188 loss_train: 0.0489 acc_train: 1.0000 loss_val: 0.5653 acc_val: 0.7820 time: 0.0276s\n",
      "Epoch: 0189 loss_train: 0.0620 acc_train: 1.0000 loss_val: 0.5651 acc_val: 0.7820 time: 0.0291s\n",
      "Epoch: 0190 loss_train: 0.0971 acc_train: 1.0000 loss_val: 0.5652 acc_val: 0.7820 time: 0.0278s\n",
      "Epoch: 0191 loss_train: 0.0744 acc_train: 1.0000 loss_val: 0.5647 acc_val: 0.7820 time: 0.0278s\n",
      "Epoch: 0192 loss_train: 0.0899 acc_train: 1.0000 loss_val: 0.5687 acc_val: 0.7800 time: 0.0280s\n",
      "Epoch: 0193 loss_train: 0.0826 acc_train: 1.0000 loss_val: 0.5718 acc_val: 0.7780 time: 0.0279s\n",
      "Epoch: 0194 loss_train: 0.1219 acc_train: 0.9833 loss_val: 0.5728 acc_val: 0.7780 time: 0.0277s\n",
      "Epoch: 0195 loss_train: 0.0718 acc_train: 0.9833 loss_val: 0.5693 acc_val: 0.7780 time: 0.0278s\n",
      "Epoch: 0196 loss_train: 0.0630 acc_train: 1.0000 loss_val: 0.5657 acc_val: 0.7720 time: 0.0279s\n",
      "Epoch: 0197 loss_train: 0.0505 acc_train: 1.0000 loss_val: 0.5626 acc_val: 0.7760 time: 0.0292s\n",
      "Epoch: 0198 loss_train: 0.0585 acc_train: 1.0000 loss_val: 0.5615 acc_val: 0.7780 time: 0.0276s\n",
      "Epoch: 0199 loss_train: 0.0646 acc_train: 1.0000 loss_val: 0.5627 acc_val: 0.7800 time: 0.0277s\n",
      "Epoch: 0200 loss_train: 0.0583 acc_train: 1.0000 loss_val: 0.5644 acc_val: 0.7860 time: 0.0278s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 5.8181s\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: argmax() got an unexpected keyword argument 'axis'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_24791/719933687.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;31m# Testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_24791/719933687.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m#loss_test = F.cross_entropy(output[idx_test], labels[idx_test])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0macc_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mf1_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1214\u001b[0m     \"\"\"\n\u001b[1;32m   1215\u001b[0m     \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'keepdims'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# Call _wrapit from within the except clause to ensure a potential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# exception has a traceback chain.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cluster/apps/nss/gcc-8.2.0/python/3.8.5/x86_64/lib64/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Training settings\n",
    "\n",
    "args = {'fastmode':False, 'no_cuda':False, 'seed':42, 'epochs':200, 'lr': 0.01, 'weight_decay':5e-4, \n",
    "        'hidden':16, 'dropout':0.5}\n",
    "\n",
    "class AttributeDict(dict):\n",
    "    __slots__ = () \n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    \n",
    "    \n",
    "args = AttributeDict(args)\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Load data\n",
    "#adj, features, labels, idx_train, idx_val, idx_test = load_data_impl1()\n",
    "data, adj, features, labels, idx_train, idx_val, idx_test = load_data('pubmed')\n",
    "print('data.edge_weight', data.edge_weight)\n",
    "\n",
    "# Model and optimizer\n",
    "\"\"\"model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\"\"\"\n",
    "\n",
    "model = GCN(num_node_features=features.shape[1],\n",
    "            hidden_dim=args.hidden,\n",
    "            num_classes=labels.max().item()+1,\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    #output = model(features, adj)\n",
    "    output = model(features, data.edge_index, data.edge_weight)\n",
    "    \n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    #loss_train = F.cross_entropy(output[idx_train], labels[idx_train])\n",
    "    \n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        #output = model(features, adj)\n",
    "        output = model(features, data.edge_index, data.edge_weight)\n",
    "    \n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    #loss_val = F.cross_entropy(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    #output = model(features, adj)\n",
    "    output = model(features, data.edge_index, data.edge_weight)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    #loss_test = F.cross_entropy(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(np.argmax(output[idx_test].detach().numpy(), axis=1))\n",
    "    f1_test = f1_score(labels[idx_test].detach().numpy(), np.argmax(output[idx_test].detach().numpy(), axis=1), average='micro')\n",
    "    \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()),\n",
    "         \"f1_score= {:.4f}\".format(f1_test.item()))\n",
    "\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9781fb32",
   "metadata": {},
   "source": [
    "# Tensorflow Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7c3ab9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "\n",
    "\n",
    "def parse_index_file(filename):\n",
    "    \"\"\"Parse index file.\"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=bool)\n",
    "\n",
    "\n",
    "PLANETOIDS = {\"cora\": \"Cora\", \"citeseer\": \"CiteSeer\", \"pubmed\": \"PubMed\"}\n",
    "\n",
    "def load_data(dataset_str):\n",
    "    \"\"\"\n",
    "    Loads input data from gcn/data directory\n",
    "\n",
    "    ind.dataset_str.x => the feature vectors of the training instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.tx => the feature vectors of the test instances as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.allx => the feature vectors of both labeled and unlabeled training instances\n",
    "        (a superset of ind.dataset_str.x) as scipy.sparse.csr.csr_matrix object;\n",
    "    ind.dataset_str.y => the one-hot labels of the labeled training instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ty => the one-hot labels of the test instances as numpy.ndarray object;\n",
    "    ind.dataset_str.ally => the labels for instances in ind.dataset_str.allx as numpy.ndarray object;\n",
    "    ind.dataset_str.graph => a dict in the format {index: [index_of_neighbor_nodes]} as collections.defaultdict\n",
    "        object;\n",
    "    ind.dataset_str.test.index => the indices of test instances in graph, for the inductive setting as list object.\n",
    "\n",
    "    All objects above must be saved using python pickle module.\n",
    "\n",
    "    :param dataset_str: Dataset name\n",
    "    :return: All data input files loaded (as well the training/test data).\n",
    "    \"\"\"\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"/cluster/home/kamara/Explain/data/{}/raw/ind.{}.{}\".format(PLANETOIDS[dataset_str], dataset_str, names[i]), 'rb') as f:\n",
    "            if sys.version_info > (3, 0):\n",
    "                objects.append(pkl.load(f, encoding='latin1'))\n",
    "            else:\n",
    "                objects.append(pkl.load(f))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"/cluster/home/kamara/Explain/data/{}/raw/ind.{}.test.index\".format(PLANETOIDS[dataset_str], dataset_str))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if dataset_str == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = range(len(y))\n",
    "    idx_val = range(len(y), len(y)+500)\n",
    "\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    return adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return sparse_to_tuple(features)\n",
    "\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model and conversion to tuple representation.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return sparse_to_tuple(adj_normalized)\n",
    "\n",
    "\n",
    "def construct_feed_dict(features, support, labels, labels_mask, placeholders):\n",
    "    \"\"\"Construct feed dictionary.\"\"\"\n",
    "    feed_dict = dict()\n",
    "    feed_dict.update({placeholders['labels']: labels})\n",
    "    feed_dict.update({placeholders['labels_mask']: labels_mask})\n",
    "    feed_dict.update({placeholders['features']: features})\n",
    "    feed_dict.update({placeholders['support'][i]: support[i] for i in range(len(support))})\n",
    "    feed_dict.update({placeholders['num_features_nonzero']: features[1].shape})\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def chebyshev_polynomials(adj, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
    "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
    "\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
    "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
    "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
    "\n",
    "    t_k = list()\n",
    "    t_k.append(sp.eye(adj.shape[0]))\n",
    "    t_k.append(scaled_laplacian)\n",
    "\n",
    "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
    "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
    "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
    "\n",
    "    for i in range(2, k+1):\n",
    "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
    "\n",
    "    return sparse_to_tuple(t_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2c96f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def uniform(shape, scale=0.05, name=None):\n",
    "    \"\"\"Uniform init.\"\"\"\n",
    "    initial = tf.random_uniform(shape, minval=-scale, maxval=scale, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def glorot(shape, name=None):\n",
    "    \"\"\"Glorot & Bengio (AISTATS 2010) init.\"\"\"\n",
    "    init_range = np.sqrt(6.0/(shape[0]+shape[1]))\n",
    "    initial = tf.random_uniform(shape, minval=-init_range, maxval=init_range, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def zeros(shape, name=None):\n",
    "    \"\"\"All zeros.\"\"\"\n",
    "    initial = tf.zeros(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)\n",
    "\n",
    "\n",
    "def ones(shape, name=None):\n",
    "    \"\"\"All ones.\"\"\"\n",
    "    initial = tf.ones(shape, dtype=tf.float32)\n",
    "    return tf.Variable(initial, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "86b19605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global unique layer ID dictionary for layer name assignment\n",
    "_LAYER_UIDS = {}\n",
    "\n",
    "\n",
    "def get_layer_uid(layer_name=''):\n",
    "    \"\"\"Helper function, assigns unique layer IDs.\"\"\"\n",
    "    if layer_name not in _LAYER_UIDS:\n",
    "        _LAYER_UIDS[layer_name] = 1\n",
    "        return 1\n",
    "    else:\n",
    "        _LAYER_UIDS[layer_name] += 1\n",
    "        return _LAYER_UIDS[layer_name]\n",
    "\n",
    "\n",
    "def sparse_dropout(x, keep_prob, noise_shape):\n",
    "    \"\"\"Dropout for sparse tensors.\"\"\"\n",
    "    random_tensor = keep_prob\n",
    "    random_tensor += tf.random_uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    pre_out = tf.sparse_retain(x, dropout_mask)\n",
    "    return pre_out * (1./keep_prob)\n",
    "\n",
    "\n",
    "def dot(x, y, sparse=False):\n",
    "    \"\"\"Wrapper for tf.matmul (sparse vs dense).\"\"\"\n",
    "    if sparse:\n",
    "        res = tf.sparse_tensor_dense_matmul(x, y)\n",
    "    else:\n",
    "        res = tf.matmul(x, y)\n",
    "    return res\n",
    "\n",
    "\n",
    "class Layer(object):\n",
    "    \"\"\"Base layer class. Defines basic API for all layer objects.\n",
    "    Implementation inspired by keras (http://keras.io).\n",
    "\n",
    "    # Properties\n",
    "        name: String, defines the variable scope of the layer.\n",
    "        logging: Boolean, switches Tensorflow histogram logging on/off\n",
    "\n",
    "    # Methods\n",
    "        _call(inputs): Defines computation graph of layer\n",
    "            (i.e. takes input, returns output)\n",
    "        __call__(inputs): Wrapper for _call()\n",
    "        _log_vars(): Log all variables\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            layer = self.__class__.__name__.lower()\n",
    "            name = layer + '_' + str(get_layer_uid(layer))\n",
    "        self.name = name\n",
    "        self.vars = {}\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "        self.sparse_inputs = False\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        return inputs\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        with tf.name_scope(self.name):\n",
    "            if self.logging and not self.sparse_inputs:\n",
    "                tf.summary.histogram(self.name + '/inputs', inputs)\n",
    "            outputs = self._call(inputs)\n",
    "            if self.logging:\n",
    "                tf.summary.histogram(self.name + '/outputs', outputs)\n",
    "            return outputs\n",
    "\n",
    "    def _log_vars(self):\n",
    "        for var in self.vars:\n",
    "            tf.summary.histogram(self.name + '/vars/' + var, self.vars[var])\n",
    "\n",
    "\n",
    "class Dense(Layer):\n",
    "    \"\"\"Dense layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0., sparse_inputs=False,\n",
    "                 act=tf.nn.relu, bias=False, featureless=False, **kwargs):\n",
    "        super(Dense, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            self.vars['weights'] = glorot([input_dim, output_dim],\n",
    "                                          name='weights')\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # transform\n",
    "        output = dot(x, self.vars['weights'], sparse=self.sparse_inputs)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)\n",
    "\n",
    "\n",
    "class GraphConvolution(Layer):\n",
    "    \"\"\"Graph convolution layer.\"\"\"\n",
    "    def __init__(self, input_dim, output_dim, placeholders, dropout=0.,\n",
    "                 sparse_inputs=False, act=tf.nn.relu, bias=False,\n",
    "                 featureless=False, **kwargs):\n",
    "        super(GraphConvolution, self).__init__(**kwargs)\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = placeholders['dropout']\n",
    "        else:\n",
    "            self.dropout = 0.\n",
    "\n",
    "        self.act = act\n",
    "        self.support = placeholders['support']\n",
    "        self.sparse_inputs = sparse_inputs\n",
    "        self.featureless = featureless\n",
    "        self.bias = bias\n",
    "\n",
    "        # helper variable for sparse dropout\n",
    "        self.num_features_nonzero = placeholders['num_features_nonzero']\n",
    "\n",
    "        with tf.variable_scope(self.name + '_vars'):\n",
    "            for i in range(len(self.support)):\n",
    "                self.vars['weights_' + str(i)] = glorot([input_dim, output_dim],\n",
    "                                                        name='weights_' + str(i))\n",
    "            if self.bias:\n",
    "                self.vars['bias'] = zeros([output_dim], name='bias')\n",
    "\n",
    "        if self.logging:\n",
    "            self._log_vars()\n",
    "\n",
    "    def _call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        # dropout\n",
    "        if self.sparse_inputs:\n",
    "            x = sparse_dropout(x, 1-self.dropout, self.num_features_nonzero)\n",
    "        else:\n",
    "            x = tf.nn.dropout(x, 1-self.dropout)\n",
    "\n",
    "        # convolve\n",
    "        supports = list()\n",
    "        for i in range(len(self.support)):\n",
    "            if not self.featureless:\n",
    "                pre_sup = dot(x, self.vars['weights_' + str(i)],\n",
    "                              sparse=self.sparse_inputs)\n",
    "            else:\n",
    "                pre_sup = self.vars['weights_' + str(i)]\n",
    "            support = dot(self.support[i], pre_sup, sparse=True)\n",
    "            supports.append(support)\n",
    "        output = tf.add_n(supports)\n",
    "\n",
    "        # bias\n",
    "        if self.bias:\n",
    "            output += self.vars['bias']\n",
    "\n",
    "        return self.act(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c548d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax_cross_entropy(preds, labels, mask):\n",
    "    \"\"\"Softmax cross-entropy loss with masking.\"\"\"\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=labels)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "def masked_accuracy(preds, labels, mask):\n",
    "    \"\"\"Accuracy with masking.\"\"\"\n",
    "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
    "    accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    mask /= tf.reduce_mean(mask)\n",
    "    accuracy_all *= mask\n",
    "    return tf.reduce_mean(accuracy_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "db970bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        allowed_kwargs = {'name', 'logging'}\n",
    "        for kwarg in kwargs.keys():\n",
    "            assert kwarg in allowed_kwargs, 'Invalid keyword argument: ' + kwarg\n",
    "        name = kwargs.get('name')\n",
    "        if not name:\n",
    "            name = self.__class__.__name__.lower()\n",
    "        self.name = name\n",
    "\n",
    "        logging = kwargs.get('logging', False)\n",
    "        self.logging = logging\n",
    "\n",
    "        self.vars = {}\n",
    "        self.placeholders = {}\n",
    "\n",
    "        self.layers = []\n",
    "        self.activations = []\n",
    "\n",
    "        self.inputs = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.loss = 0\n",
    "        self.accuracy = 0\n",
    "        self.optimizer = None\n",
    "        self.opt_op = None\n",
    "\n",
    "    def _build(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\" Wrapper for _build() \"\"\"\n",
    "        with tf.variable_scope(self.name):\n",
    "            self._build()\n",
    "\n",
    "        # Build sequential layer model\n",
    "        self.activations.append(self.inputs)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(self.activations[-1])\n",
    "            self.activations.append(hidden)\n",
    "        self.outputs = self.activations[-1]\n",
    "\n",
    "        # Store model variables for easy access\n",
    "        variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=self.name)\n",
    "        self.vars = {var.name: var for var in variables}\n",
    "\n",
    "        # Build metrics\n",
    "        self._loss()\n",
    "        self._accuracy()\n",
    "\n",
    "        self.opt_op = self.optimizer.minimize(self.loss)\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def _loss(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _accuracy(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def save(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = saver.save(sess, \"tmp/%s.ckpt\" % self.name)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "    def load(self, sess=None):\n",
    "        if not sess:\n",
    "            raise AttributeError(\"TensorFlow session not provided.\")\n",
    "        saver = tf.train.Saver(self.vars)\n",
    "        save_path = \"tmp/%s.ckpt\" % self.name\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored from file: %s\" % save_path)\n",
    "\n",
    "\n",
    "class MLP(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(MLP, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "        self.layers.append(Dense(input_dim=self.input_dim,\n",
    "                                 output_dim=FLAGS.hidden1,\n",
    "                                 placeholders=self.placeholders,\n",
    "                                 act=tf.nn.relu,\n",
    "                                 dropout=True,\n",
    "                                 sparse_inputs=True,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "        self.layers.append(Dense(input_dim=FLAGS.hidden1,\n",
    "                                 output_dim=self.output_dim,\n",
    "                                 placeholders=self.placeholders,\n",
    "                                 act=lambda x: x,\n",
    "                                 dropout=True,\n",
    "                                 logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)\n",
    "\n",
    "\n",
    "class GCN(Model):\n",
    "    def __init__(self, placeholders, input_dim, **kwargs):\n",
    "        super(GCN, self).__init__(**kwargs)\n",
    "\n",
    "        self.inputs = placeholders['features']\n",
    "        self.input_dim = input_dim\n",
    "        # self.input_dim = self.inputs.get_shape().as_list()[1]  # To be supported in future Tensorflow versions\n",
    "        self.output_dim = placeholders['labels'].get_shape().as_list()[1]\n",
    "        self.placeholders = placeholders\n",
    "\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=FLAGS.learning_rate)\n",
    "\n",
    "        self.build()\n",
    "\n",
    "    def _loss(self):\n",
    "        # Weight decay loss\n",
    "        for var in self.layers[0].vars.values():\n",
    "            self.loss += FLAGS.weight_decay * tf.nn.l2_loss(var)\n",
    "\n",
    "        # Cross entropy error\n",
    "        self.loss += masked_softmax_cross_entropy(self.outputs, self.placeholders['labels'],\n",
    "                                                  self.placeholders['labels_mask'])\n",
    "\n",
    "    def _accuracy(self):\n",
    "        self.accuracy = masked_accuracy(self.outputs, self.placeholders['labels'],\n",
    "                                        self.placeholders['labels_mask'])\n",
    "\n",
    "    def _build(self):\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=self.input_dim,\n",
    "                                            output_dim=FLAGS.hidden1,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=tf.nn.relu,\n",
    "                                            dropout=True,\n",
    "                                            sparse_inputs=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "        self.layers.append(GraphConvolution(input_dim=FLAGS.hidden1,\n",
    "                                            output_dim=self.output_dim,\n",
    "                                            placeholders=self.placeholders,\n",
    "                                            act=lambda x: x,\n",
    "                                            dropout=True,\n",
    "                                            logging=self.logging))\n",
    "\n",
    "    def predict(self):\n",
    "        return tf.nn.softmax(self.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5c2d2756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 train_loss= 1.10520 train_acc= 0.41667 val_loss= 1.10238 val_acc= 0.58000 time= 0.23617\n",
      "Epoch: 0002 train_loss= 1.09978 train_acc= 0.61667 val_loss= 1.09789 val_acc= 0.68400 time= 0.12964\n",
      "Epoch: 0003 train_loss= 1.09100 train_acc= 0.75000 val_loss= 1.09320 val_acc= 0.68600 time= 0.13018\n",
      "Epoch: 0004 train_loss= 1.08477 train_acc= 0.78333 val_loss= 1.08842 val_acc= 0.71200 time= 0.13211\n",
      "Epoch: 0005 train_loss= 1.07603 train_acc= 0.80000 val_loss= 1.08370 val_acc= 0.73000 time= 0.14324\n",
      "Epoch: 0006 train_loss= 1.06793 train_acc= 0.81667 val_loss= 1.07887 val_acc= 0.73000 time= 0.13176\n",
      "Epoch: 0007 train_loss= 1.06125 train_acc= 0.78333 val_loss= 1.07407 val_acc= 0.74000 time= 0.13217\n",
      "Epoch: 0008 train_loss= 1.04720 train_acc= 0.81667 val_loss= 1.06930 val_acc= 0.74000 time= 0.12774\n",
      "Epoch: 0009 train_loss= 1.04453 train_acc= 0.85000 val_loss= 1.06452 val_acc= 0.73600 time= 0.13147\n",
      "Epoch: 0010 train_loss= 1.02949 train_acc= 0.86667 val_loss= 1.05968 val_acc= 0.73200 time= 0.12790\n",
      "Epoch: 0011 train_loss= 1.02843 train_acc= 0.80000 val_loss= 1.05497 val_acc= 0.73000 time= 0.12841\n",
      "Epoch: 0012 train_loss= 1.00887 train_acc= 0.81667 val_loss= 1.05014 val_acc= 0.72400 time= 0.12738\n",
      "Epoch: 0013 train_loss= 1.00619 train_acc= 0.86667 val_loss= 1.04523 val_acc= 0.72600 time= 0.12827\n",
      "Epoch: 0014 train_loss= 0.99154 train_acc= 0.85000 val_loss= 1.04027 val_acc= 0.72800 time= 0.12994\n",
      "Epoch: 0015 train_loss= 0.98823 train_acc= 0.80000 val_loss= 1.03529 val_acc= 0.72600 time= 0.13766\n",
      "Epoch: 0016 train_loss= 0.97393 train_acc= 0.85000 val_loss= 1.03033 val_acc= 0.72800 time= 0.13106\n",
      "Epoch: 0017 train_loss= 0.96046 train_acc= 0.85000 val_loss= 1.02527 val_acc= 0.72600 time= 0.13125\n",
      "Epoch: 0018 train_loss= 0.95651 train_acc= 0.90000 val_loss= 1.02024 val_acc= 0.72800 time= 0.13209\n",
      "Epoch: 0019 train_loss= 0.93206 train_acc= 0.91667 val_loss= 1.01506 val_acc= 0.73200 time= 0.12971\n",
      "Epoch: 0020 train_loss= 0.93161 train_acc= 0.85000 val_loss= 1.00986 val_acc= 0.72800 time= 0.12887\n",
      "Epoch: 0021 train_loss= 0.91885 train_acc= 0.90000 val_loss= 1.00462 val_acc= 0.72800 time= 0.12824\n",
      "Epoch: 0022 train_loss= 0.90346 train_acc= 0.88333 val_loss= 0.99914 val_acc= 0.73400 time= 0.13085\n",
      "Epoch: 0023 train_loss= 0.91576 train_acc= 0.86667 val_loss= 0.99366 val_acc= 0.73800 time= 0.13059\n",
      "Epoch: 0024 train_loss= 0.88616 train_acc= 0.81667 val_loss= 0.98828 val_acc= 0.73400 time= 0.12893\n",
      "Epoch: 0025 train_loss= 0.87507 train_acc= 0.86667 val_loss= 0.98291 val_acc= 0.73800 time= 0.12887\n",
      "Epoch: 0026 train_loss= 0.85489 train_acc= 0.90000 val_loss= 0.97758 val_acc= 0.74000 time= 0.13085\n",
      "Epoch: 0027 train_loss= 0.86121 train_acc= 0.90000 val_loss= 0.97226 val_acc= 0.73600 time= 0.13011\n",
      "Epoch: 0028 train_loss= 0.82844 train_acc= 0.88333 val_loss= 0.96693 val_acc= 0.73800 time= 0.12964\n",
      "Epoch: 0029 train_loss= 0.84395 train_acc= 0.86667 val_loss= 0.96174 val_acc= 0.73800 time= 0.12889\n",
      "Epoch: 0030 train_loss= 0.81586 train_acc= 0.90000 val_loss= 0.95672 val_acc= 0.74000 time= 0.13090\n",
      "Epoch: 0031 train_loss= 0.81718 train_acc= 0.86667 val_loss= 0.95166 val_acc= 0.74000 time= 0.13119\n",
      "Epoch: 0032 train_loss= 0.80397 train_acc= 0.88333 val_loss= 0.94646 val_acc= 0.74200 time= 0.12920\n",
      "Epoch: 0033 train_loss= 0.78192 train_acc= 0.91667 val_loss= 0.94107 val_acc= 0.74400 time= 0.13266\n",
      "Epoch: 0034 train_loss= 0.77749 train_acc= 0.93333 val_loss= 0.93585 val_acc= 0.74200 time= 0.12892\n",
      "Epoch: 0035 train_loss= 0.79680 train_acc= 0.81667 val_loss= 0.93065 val_acc= 0.74400 time= 0.12868\n",
      "Epoch: 0036 train_loss= 0.75755 train_acc= 0.90000 val_loss= 0.92556 val_acc= 0.74400 time= 0.12760\n",
      "Epoch: 0037 train_loss= 0.74335 train_acc= 0.91667 val_loss= 0.92040 val_acc= 0.75000 time= 0.13120\n",
      "Epoch: 0038 train_loss= 0.73513 train_acc= 0.90000 val_loss= 0.91538 val_acc= 0.74800 time= 0.12897\n",
      "Epoch: 0039 train_loss= 0.71679 train_acc= 0.93333 val_loss= 0.91047 val_acc= 0.74800 time= 0.13021\n",
      "Epoch: 0040 train_loss= 0.69129 train_acc= 0.91667 val_loss= 0.90577 val_acc= 0.75200 time= 0.12844\n",
      "Epoch: 0041 train_loss= 0.70571 train_acc= 0.93333 val_loss= 0.90126 val_acc= 0.75000 time= 0.13260\n",
      "Epoch: 0042 train_loss= 0.70097 train_acc= 0.93333 val_loss= 0.89668 val_acc= 0.75400 time= 0.12859\n",
      "Epoch: 0043 train_loss= 0.68858 train_acc= 0.91667 val_loss= 0.89226 val_acc= 0.75400 time= 0.13069\n",
      "Epoch: 0044 train_loss= 0.68164 train_acc= 0.93333 val_loss= 0.88789 val_acc= 0.75800 time= 0.13280\n",
      "Epoch: 0045 train_loss= 0.67534 train_acc= 0.91667 val_loss= 0.88336 val_acc= 0.76000 time= 0.13141\n",
      "Epoch: 0046 train_loss= 0.67188 train_acc= 0.91667 val_loss= 0.87882 val_acc= 0.76000 time= 0.13212\n",
      "Epoch: 0047 train_loss= 0.66175 train_acc= 0.90000 val_loss= 0.87454 val_acc= 0.76400 time= 0.13020\n",
      "Epoch: 0048 train_loss= 0.64992 train_acc= 0.96667 val_loss= 0.87035 val_acc= 0.76600 time= 0.13031\n",
      "Epoch: 0049 train_loss= 0.66886 train_acc= 0.91667 val_loss= 0.86643 val_acc= 0.76600 time= 0.13136\n",
      "Epoch: 0050 train_loss= 0.64228 train_acc= 0.95000 val_loss= 0.86263 val_acc= 0.76600 time= 0.13252\n",
      "Epoch: 0051 train_loss= 0.64292 train_acc= 0.93333 val_loss= 0.85892 val_acc= 0.76600 time= 0.13005\n",
      "Epoch: 0052 train_loss= 0.63079 train_acc= 0.96667 val_loss= 0.85541 val_acc= 0.76800 time= 0.13043\n",
      "Epoch: 0053 train_loss= 0.62207 train_acc= 0.90000 val_loss= 0.85200 val_acc= 0.76800 time= 0.13368\n",
      "Epoch: 0054 train_loss= 0.61548 train_acc= 0.90000 val_loss= 0.84856 val_acc= 0.76800 time= 0.13033\n",
      "Epoch: 0055 train_loss= 0.59933 train_acc= 0.93333 val_loss= 0.84506 val_acc= 0.76400 time= 0.13158\n",
      "Epoch: 0056 train_loss= 0.60722 train_acc= 0.95000 val_loss= 0.84168 val_acc= 0.76400 time= 0.12856\n",
      "Epoch: 0057 train_loss= 0.58228 train_acc= 0.95000 val_loss= 0.83834 val_acc= 0.76800 time= 0.12985\n",
      "Epoch: 0058 train_loss= 0.60440 train_acc= 0.95000 val_loss= 0.83484 val_acc= 0.77400 time= 0.12798\n",
      "Epoch: 0059 train_loss= 0.58183 train_acc= 0.95000 val_loss= 0.83164 val_acc= 0.77600 time= 0.13012\n",
      "Epoch: 0060 train_loss= 0.57040 train_acc= 0.95000 val_loss= 0.82856 val_acc= 0.77600 time= 0.12919\n",
      "Epoch: 0061 train_loss= 0.57955 train_acc= 0.93333 val_loss= 0.82560 val_acc= 0.77600 time= 0.13076\n",
      "Epoch: 0062 train_loss= 0.54755 train_acc= 0.96667 val_loss= 0.82273 val_acc= 0.77600 time= 0.13111\n",
      "Epoch: 0063 train_loss= 0.59941 train_acc= 0.95000 val_loss= 0.81980 val_acc= 0.77600 time= 0.13224\n",
      "Epoch: 0064 train_loss= 0.58287 train_acc= 0.91667 val_loss= 0.81690 val_acc= 0.78000 time= 0.13042\n",
      "Epoch: 0065 train_loss= 0.54449 train_acc= 0.95000 val_loss= 0.81423 val_acc= 0.78200 time= 0.13050\n",
      "Epoch: 0066 train_loss= 0.55310 train_acc= 0.95000 val_loss= 0.81151 val_acc= 0.78200 time= 0.13047\n",
      "Epoch: 0067 train_loss= 0.52979 train_acc= 0.95000 val_loss= 0.80877 val_acc= 0.78200 time= 0.12786\n",
      "Epoch: 0068 train_loss= 0.54960 train_acc= 0.93333 val_loss= 0.80596 val_acc= 0.78600 time= 0.13190\n",
      "Epoch: 0069 train_loss= 0.51364 train_acc= 0.98333 val_loss= 0.80312 val_acc= 0.78800 time= 0.13100\n",
      "Epoch: 0070 train_loss= 0.50666 train_acc= 0.95000 val_loss= 0.80038 val_acc= 0.78800 time= 0.13452\n",
      "Epoch: 0071 train_loss= 0.51898 train_acc= 0.95000 val_loss= 0.79798 val_acc= 0.79000 time= 0.12909\n",
      "Epoch: 0072 train_loss= 0.49624 train_acc= 0.93333 val_loss= 0.79570 val_acc= 0.79000 time= 0.12886\n",
      "Epoch: 0073 train_loss= 0.49415 train_acc= 0.93333 val_loss= 0.79369 val_acc= 0.79200 time= 0.12957\n",
      "Epoch: 0074 train_loss= 0.49452 train_acc= 0.93333 val_loss= 0.79192 val_acc= 0.79600 time= 0.12866\n",
      "Epoch: 0075 train_loss= 0.49203 train_acc= 0.95000 val_loss= 0.79027 val_acc= 0.79800 time= 0.12882\n",
      "Epoch: 0076 train_loss= 0.47840 train_acc= 0.96667 val_loss= 0.78889 val_acc= 0.80200 time= 0.12944\n",
      "Epoch: 0077 train_loss= 0.50823 train_acc= 0.95000 val_loss= 0.78752 val_acc= 0.80400 time= 0.12894\n",
      "Epoch: 0078 train_loss= 0.48449 train_acc= 0.98333 val_loss= 0.78625 val_acc= 0.80400 time= 0.13054\n",
      "Epoch: 0079 train_loss= 0.47656 train_acc= 0.96667 val_loss= 0.78474 val_acc= 0.79800 time= 0.13189\n",
      "Epoch: 0080 train_loss= 0.49978 train_acc= 0.96667 val_loss= 0.78341 val_acc= 0.79800 time= 0.12919\n",
      "Epoch: 0081 train_loss= 0.45243 train_acc= 0.98333 val_loss= 0.78209 val_acc= 0.80000 time= 0.12945\n",
      "Epoch: 0082 train_loss= 0.45906 train_acc= 0.95000 val_loss= 0.78079 val_acc= 0.80200 time= 0.12896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0083 train_loss= 0.48599 train_acc= 0.93333 val_loss= 0.77970 val_acc= 0.80200 time= 0.12943\n",
      "Epoch: 0084 train_loss= 0.48562 train_acc= 0.95000 val_loss= 0.77862 val_acc= 0.80000 time= 0.12852\n",
      "Epoch: 0085 train_loss= 0.47162 train_acc= 0.95000 val_loss= 0.77748 val_acc= 0.80000 time= 0.12953\n",
      "Epoch: 0086 train_loss= 0.49713 train_acc= 0.95000 val_loss= 0.77597 val_acc= 0.80000 time= 0.13119\n",
      "Epoch: 0087 train_loss= 0.46257 train_acc= 0.95000 val_loss= 0.77386 val_acc= 0.79800 time= 0.12933\n",
      "Epoch: 0088 train_loss= 0.44254 train_acc= 0.95000 val_loss= 0.77184 val_acc= 0.80000 time= 0.12827\n",
      "Epoch: 0089 train_loss= 0.46816 train_acc= 0.95000 val_loss= 0.76982 val_acc= 0.80400 time= 0.12867\n",
      "Epoch: 0090 train_loss= 0.47945 train_acc= 0.95000 val_loss= 0.76780 val_acc= 0.80600 time= 0.12805\n",
      "Epoch: 0091 train_loss= 0.47900 train_acc= 0.93333 val_loss= 0.76607 val_acc= 0.80400 time= 0.12896\n",
      "Epoch: 0092 train_loss= 0.47117 train_acc= 0.96667 val_loss= 0.76432 val_acc= 0.80400 time= 0.12905\n",
      "Epoch: 0093 train_loss= 0.44871 train_acc= 0.96667 val_loss= 0.76272 val_acc= 0.80600 time= 0.12818\n",
      "Epoch: 0094 train_loss= 0.41796 train_acc= 1.00000 val_loss= 0.76134 val_acc= 0.80400 time= 0.13169\n",
      "Epoch: 0095 train_loss= 0.39461 train_acc= 0.98333 val_loss= 0.76012 val_acc= 0.80200 time= 0.12921\n",
      "Epoch: 0096 train_loss= 0.42834 train_acc= 0.98333 val_loss= 0.75926 val_acc= 0.80000 time= 0.12918\n",
      "Epoch: 0097 train_loss= 0.41439 train_acc= 0.95000 val_loss= 0.75828 val_acc= 0.80000 time= 0.12815\n",
      "Epoch: 0098 train_loss= 0.43926 train_acc= 0.98333 val_loss= 0.75759 val_acc= 0.80000 time= 0.13289\n",
      "Epoch: 0099 train_loss= 0.40112 train_acc= 0.96667 val_loss= 0.75709 val_acc= 0.80000 time= 0.12957\n",
      "Epoch: 0100 train_loss= 0.42713 train_acc= 0.96667 val_loss= 0.75678 val_acc= 0.79600 time= 0.12935\n",
      "Epoch: 0101 train_loss= 0.40312 train_acc= 0.98333 val_loss= 0.75641 val_acc= 0.79600 time= 0.12925\n",
      "Epoch: 0102 train_loss= 0.43336 train_acc= 0.96667 val_loss= 0.75612 val_acc= 0.79600 time= 0.12968\n",
      "Epoch: 0103 train_loss= 0.43479 train_acc= 0.96667 val_loss= 0.75544 val_acc= 0.79800 time= 0.13176\n",
      "Epoch: 0104 train_loss= 0.40537 train_acc= 0.98333 val_loss= 0.75455 val_acc= 0.79600 time= 0.13093\n",
      "Epoch: 0105 train_loss= 0.39091 train_acc= 0.98333 val_loss= 0.75371 val_acc= 0.79600 time= 0.13228\n",
      "Epoch: 0106 train_loss= 0.39552 train_acc= 0.98333 val_loss= 0.75231 val_acc= 0.79600 time= 0.13069\n",
      "Epoch: 0107 train_loss= 0.38909 train_acc= 0.96667 val_loss= 0.75098 val_acc= 0.79600 time= 0.13324\n",
      "Epoch: 0108 train_loss= 0.38793 train_acc= 1.00000 val_loss= 0.74966 val_acc= 0.79600 time= 0.13224\n",
      "Epoch: 0109 train_loss= 0.38279 train_acc= 1.00000 val_loss= 0.74825 val_acc= 0.79600 time= 0.13502\n",
      "Epoch: 0110 train_loss= 0.39738 train_acc= 0.96667 val_loss= 0.74725 val_acc= 0.79400 time= 0.13099\n",
      "Epoch: 0111 train_loss= 0.40076 train_acc= 0.98333 val_loss= 0.74603 val_acc= 0.79200 time= 0.13467\n",
      "Epoch: 0112 train_loss= 0.38820 train_acc= 1.00000 val_loss= 0.74462 val_acc= 0.79400 time= 0.13986\n",
      "Epoch: 0113 train_loss= 0.44218 train_acc= 0.98333 val_loss= 0.74344 val_acc= 0.79200 time= 0.13769\n",
      "Epoch: 0114 train_loss= 0.37890 train_acc= 1.00000 val_loss= 0.74212 val_acc= 0.79200 time= 0.12906\n",
      "Epoch: 0115 train_loss= 0.44063 train_acc= 0.95000 val_loss= 0.74028 val_acc= 0.79200 time= 0.12855\n",
      "Epoch: 0116 train_loss= 0.42519 train_acc= 0.96667 val_loss= 0.73878 val_acc= 0.79200 time= 0.12902\n",
      "Epoch: 0117 train_loss= 0.37419 train_acc= 0.96667 val_loss= 0.73772 val_acc= 0.79200 time= 0.13304\n",
      "Epoch: 0118 train_loss= 0.38680 train_acc= 0.95000 val_loss= 0.73669 val_acc= 0.79200 time= 0.13239\n",
      "Epoch: 0119 train_loss= 0.39409 train_acc= 0.98333 val_loss= 0.73584 val_acc= 0.79200 time= 0.12979\n",
      "Epoch: 0120 train_loss= 0.37174 train_acc= 1.00000 val_loss= 0.73506 val_acc= 0.79200 time= 0.12980\n",
      "Epoch: 0121 train_loss= 0.42429 train_acc= 0.95000 val_loss= 0.73446 val_acc= 0.79200 time= 0.12956\n",
      "Epoch: 0122 train_loss= 0.36381 train_acc= 0.96667 val_loss= 0.73386 val_acc= 0.79200 time= 0.12914\n",
      "Epoch: 0123 train_loss= 0.38730 train_acc= 0.98333 val_loss= 0.73293 val_acc= 0.79200 time= 0.12844\n",
      "Epoch: 0124 train_loss= 0.37439 train_acc= 0.96667 val_loss= 0.73263 val_acc= 0.79200 time= 0.12903\n",
      "Epoch: 0125 train_loss= 0.34649 train_acc= 1.00000 val_loss= 0.73235 val_acc= 0.79200 time= 0.12992\n",
      "Epoch: 0126 train_loss= 0.42231 train_acc= 0.96667 val_loss= 0.73212 val_acc= 0.79400 time= 0.13251\n",
      "Epoch: 0127 train_loss= 0.40459 train_acc= 0.96667 val_loss= 0.73139 val_acc= 0.79400 time= 0.13145\n",
      "Epoch: 0128 train_loss= 0.40255 train_acc= 0.93333 val_loss= 0.73037 val_acc= 0.79400 time= 0.12825\n",
      "Epoch: 0129 train_loss= 0.36134 train_acc= 0.96667 val_loss= 0.72939 val_acc= 0.79400 time= 0.12990\n",
      "Epoch: 0130 train_loss= 0.41096 train_acc= 0.96667 val_loss= 0.72865 val_acc= 0.79600 time= 0.12825\n",
      "Epoch: 0131 train_loss= 0.37625 train_acc= 0.98333 val_loss= 0.72841 val_acc= 0.79600 time= 0.12945\n",
      "Epoch: 0132 train_loss= 0.35348 train_acc= 0.96667 val_loss= 0.72775 val_acc= 0.79600 time= 0.12791\n",
      "Epoch: 0133 train_loss= 0.35862 train_acc= 0.96667 val_loss= 0.72701 val_acc= 0.79600 time= 0.12852\n",
      "Epoch: 0134 train_loss= 0.42466 train_acc= 0.95000 val_loss= 0.72676 val_acc= 0.79600 time= 0.13227\n",
      "Epoch: 0135 train_loss= 0.36141 train_acc= 0.95000 val_loss= 0.72622 val_acc= 0.79600 time= 0.13326\n",
      "Epoch: 0136 train_loss= 0.37722 train_acc= 0.96667 val_loss= 0.72570 val_acc= 0.79600 time= 0.12972\n",
      "Epoch: 0137 train_loss= 0.37883 train_acc= 0.96667 val_loss= 0.72500 val_acc= 0.79400 time= 0.12919\n",
      "Epoch: 0138 train_loss= 0.36186 train_acc= 0.98333 val_loss= 0.72442 val_acc= 0.79400 time= 0.13127\n",
      "Epoch: 0139 train_loss= 0.40865 train_acc= 0.98333 val_loss= 0.72397 val_acc= 0.79000 time= 0.13229\n",
      "Epoch: 0140 train_loss= 0.36350 train_acc= 0.96667 val_loss= 0.72322 val_acc= 0.79400 time= 0.12839\n",
      "Epoch: 0141 train_loss= 0.36279 train_acc= 0.96667 val_loss= 0.72257 val_acc= 0.79400 time= 0.12912\n",
      "Epoch: 0142 train_loss= 0.35945 train_acc= 0.96667 val_loss= 0.72219 val_acc= 0.79400 time= 0.13046\n",
      "Epoch: 0143 train_loss= 0.33400 train_acc= 0.98333 val_loss= 0.72147 val_acc= 0.79400 time= 0.12960\n",
      "Epoch: 0144 train_loss= 0.35701 train_acc= 1.00000 val_loss= 0.72073 val_acc= 0.79400 time= 0.12787\n",
      "Epoch: 0145 train_loss= 0.35402 train_acc= 0.98333 val_loss= 0.71994 val_acc= 0.79200 time= 0.12942\n",
      "Epoch: 0146 train_loss= 0.31950 train_acc= 0.98333 val_loss= 0.71923 val_acc= 0.79200 time= 0.13476\n",
      "Epoch: 0147 train_loss= 0.34799 train_acc= 0.96667 val_loss= 0.71813 val_acc= 0.79400 time= 0.12932\n",
      "Epoch: 0148 train_loss= 0.35186 train_acc= 0.98333 val_loss= 0.71739 val_acc= 0.79400 time= 0.13054\n",
      "Epoch: 0149 train_loss= 0.31476 train_acc= 1.00000 val_loss= 0.71653 val_acc= 0.79400 time= 0.13138\n",
      "Epoch: 0150 train_loss= 0.35535 train_acc= 0.98333 val_loss= 0.71592 val_acc= 0.79600 time= 0.13141\n",
      "Epoch: 0151 train_loss= 0.34899 train_acc= 0.96667 val_loss= 0.71573 val_acc= 0.79600 time= 0.12831\n",
      "Epoch: 0152 train_loss= 0.34274 train_acc= 1.00000 val_loss= 0.71500 val_acc= 0.79800 time= 0.12800\n",
      "Epoch: 0153 train_loss= 0.29916 train_acc= 1.00000 val_loss= 0.71409 val_acc= 0.79600 time= 0.13044\n",
      "Epoch: 0154 train_loss= 0.33981 train_acc= 1.00000 val_loss= 0.71318 val_acc= 0.79400 time= 0.12790\n",
      "Epoch: 0155 train_loss= 0.37825 train_acc= 1.00000 val_loss= 0.71234 val_acc= 0.79400 time= 0.12967\n",
      "Epoch: 0156 train_loss= 0.33482 train_acc= 0.96667 val_loss= 0.71107 val_acc= 0.79600 time= 0.13010\n",
      "Epoch: 0157 train_loss= 0.33013 train_acc= 0.98333 val_loss= 0.70980 val_acc= 0.79600 time= 0.12850\n",
      "Epoch: 0158 train_loss= 0.33121 train_acc= 1.00000 val_loss= 0.70862 val_acc= 0.79600 time= 0.13074\n",
      "Epoch: 0159 train_loss= 0.31036 train_acc= 1.00000 val_loss= 0.70737 val_acc= 0.79600 time= 0.12859\n",
      "Epoch: 0160 train_loss= 0.32598 train_acc= 0.98333 val_loss= 0.70607 val_acc= 0.80000 time= 0.13146\n",
      "Epoch: 0161 train_loss= 0.34468 train_acc= 0.96667 val_loss= 0.70479 val_acc= 0.80200 time= 0.13002\n",
      "Epoch: 0162 train_loss= 0.30812 train_acc= 0.98333 val_loss= 0.70402 val_acc= 0.80200 time= 0.12812\n",
      "Epoch: 0163 train_loss= 0.33459 train_acc= 0.98333 val_loss= 0.70349 val_acc= 0.79800 time= 0.12902\n",
      "Epoch: 0164 train_loss= 0.34231 train_acc= 0.98333 val_loss= 0.70282 val_acc= 0.79800 time= 0.13034\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0165 train_loss= 0.31391 train_acc= 0.96667 val_loss= 0.70229 val_acc= 0.79600 time= 0.12912\n",
      "Epoch: 0166 train_loss= 0.35556 train_acc= 0.98333 val_loss= 0.70155 val_acc= 0.79600 time= 0.12965\n",
      "Epoch: 0167 train_loss= 0.28608 train_acc= 1.00000 val_loss= 0.70065 val_acc= 0.79600 time= 0.12879\n",
      "Epoch: 0168 train_loss= 0.29982 train_acc= 0.98333 val_loss= 0.70010 val_acc= 0.80400 time= 0.12901\n",
      "Epoch: 0169 train_loss= 0.33092 train_acc= 0.95000 val_loss= 0.69958 val_acc= 0.80400 time= 0.12866\n",
      "Epoch: 0170 train_loss= 0.32059 train_acc= 0.98333 val_loss= 0.69892 val_acc= 0.80400 time= 0.12871\n",
      "Epoch: 0171 train_loss= 0.32503 train_acc= 0.98333 val_loss= 0.69851 val_acc= 0.80000 time= 0.13016\n",
      "Epoch: 0172 train_loss= 0.32099 train_acc= 0.98333 val_loss= 0.69858 val_acc= 0.80400 time= 0.12761\n",
      "Epoch: 0173 train_loss= 0.33926 train_acc= 1.00000 val_loss= 0.69867 val_acc= 0.80000 time= 0.12909\n",
      "Epoch: 0174 train_loss= 0.34405 train_acc= 0.95000 val_loss= 0.69891 val_acc= 0.79800 time= 0.13018\n",
      "Epoch: 0175 train_loss= 0.34714 train_acc= 0.98333 val_loss= 0.69868 val_acc= 0.79800 time= 0.12934\n",
      "Epoch: 0176 train_loss= 0.30687 train_acc= 1.00000 val_loss= 0.69833 val_acc= 0.80000 time= 0.12777\n",
      "Epoch: 0177 train_loss= 0.28264 train_acc= 1.00000 val_loss= 0.69811 val_acc= 0.80000 time= 0.13111\n",
      "Epoch: 0178 train_loss= 0.31339 train_acc= 1.00000 val_loss= 0.69763 val_acc= 0.80000 time= 0.13237\n",
      "Epoch: 0179 train_loss= 0.32861 train_acc= 0.98333 val_loss= 0.69746 val_acc= 0.80200 time= 0.12973\n",
      "Epoch: 0180 train_loss= 0.30191 train_acc= 1.00000 val_loss= 0.69758 val_acc= 0.80200 time= 0.12875\n",
      "Epoch: 0181 train_loss= 0.30869 train_acc= 1.00000 val_loss= 0.69730 val_acc= 0.80000 time= 0.12931\n",
      "Epoch: 0182 train_loss= 0.28094 train_acc= 0.98333 val_loss= 0.69697 val_acc= 0.79800 time= 0.13098\n",
      "Epoch: 0183 train_loss= 0.29768 train_acc= 0.98333 val_loss= 0.69665 val_acc= 0.79800 time= 0.12885\n",
      "Epoch: 0184 train_loss= 0.27327 train_acc= 1.00000 val_loss= 0.69595 val_acc= 0.79800 time= 0.12815\n",
      "Epoch: 0185 train_loss= 0.30260 train_acc= 0.98333 val_loss= 0.69467 val_acc= 0.79800 time= 0.13077\n",
      "Epoch: 0186 train_loss= 0.32074 train_acc= 1.00000 val_loss= 0.69355 val_acc= 0.79800 time= 0.12924\n",
      "Epoch: 0187 train_loss= 0.29979 train_acc= 1.00000 val_loss= 0.69202 val_acc= 0.79800 time= 0.13142\n",
      "Epoch: 0188 train_loss= 0.25891 train_acc= 1.00000 val_loss= 0.69049 val_acc= 0.80000 time= 0.12856\n",
      "Epoch: 0189 train_loss= 0.28510 train_acc= 1.00000 val_loss= 0.68890 val_acc= 0.80000 time= 0.13043\n",
      "Epoch: 0190 train_loss= 0.28628 train_acc= 1.00000 val_loss= 0.68686 val_acc= 0.80000 time= 0.13137\n",
      "Epoch: 0191 train_loss= 0.28611 train_acc= 0.96667 val_loss= 0.68545 val_acc= 0.80000 time= 0.12948\n",
      "Epoch: 0192 train_loss= 0.30399 train_acc= 1.00000 val_loss= 0.68401 val_acc= 0.80000 time= 0.12880\n",
      "Epoch: 0193 train_loss= 0.28278 train_acc= 1.00000 val_loss= 0.68270 val_acc= 0.80000 time= 0.12971\n",
      "Epoch: 0194 train_loss= 0.31105 train_acc= 1.00000 val_loss= 0.68192 val_acc= 0.80200 time= 0.12868\n",
      "Epoch: 0195 train_loss= 0.31551 train_acc= 0.96667 val_loss= 0.68129 val_acc= 0.80200 time= 0.12911\n",
      "Epoch: 0196 train_loss= 0.29605 train_acc= 1.00000 val_loss= 0.68112 val_acc= 0.80000 time= 0.13248\n",
      "Epoch: 0197 train_loss= 0.29867 train_acc= 0.98333 val_loss= 0.68140 val_acc= 0.80000 time= 0.12937\n",
      "Epoch: 0198 train_loss= 0.31670 train_acc= 0.96667 val_loss= 0.68172 val_acc= 0.79800 time= 0.13059\n",
      "Epoch: 0199 train_loss= 0.30684 train_acc= 1.00000 val_loss= 0.68232 val_acc= 0.79400 time= 0.13292\n",
      "Epoch: 0200 train_loss= 0.28808 train_acc= 0.98333 val_loss= 0.68352 val_acc= 0.79400 time= 0.12833\n",
      "Early stopping...\n",
      "Optimization Finished!\n",
      "Test set results: cost= 0.69809 accuracy= 0.79400 time= 0.06859\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "\n",
    "# Set random seed\n",
    "seed = 41\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Settings\n",
    "FLAGS = {'dataset': 'pubmed', 'model':'gcn', 'learning_rate':0.01, 'epochs':200, 'hidden1':16, 'dropout':0.5,\n",
    "        'weight_decay': 5e-4, 'early_stopping':10, 'max_degree':3}\n",
    "\n",
    "class AttributeDict(dict):\n",
    "    __slots__ = () \n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    \n",
    "    \n",
    "FLAGS = AttributeDict(FLAGS)\n",
    "\n",
    "# Load data\n",
    "adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask = load_data(FLAGS.dataset)\n",
    "\n",
    "# Some preprocessing\n",
    "features = preprocess_features(features)\n",
    "if FLAGS.model == 'gcn':\n",
    "    support = [preprocess_adj(adj)]\n",
    "    num_supports = 1\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'gcn_cheby':\n",
    "    support = chebyshev_polynomials(adj, FLAGS.max_degree)\n",
    "    num_supports = 1 + FLAGS.max_degree\n",
    "    model_func = GCN\n",
    "elif FLAGS.model == 'dense':\n",
    "    support = [preprocess_adj(adj)]  # Not used\n",
    "    num_supports = 1\n",
    "    model_func = MLP\n",
    "else:\n",
    "    raise ValueError('Invalid argument for model: ' + str(FLAGS.model))\n",
    "\n",
    "# Define placeholders\n",
    "placeholders = {\n",
    "    'support': [tf.sparse_placeholder(tf.float32) for _ in range(num_supports)],\n",
    "    'features': tf.sparse_placeholder(tf.float32, shape=tf.constant(features[2], dtype=tf.int64)),\n",
    "    'labels': tf.placeholder(tf.float32, shape=(None, y_train.shape[1])),\n",
    "    'labels_mask': tf.placeholder(tf.int32),\n",
    "    'dropout': tf.placeholder_with_default(0., shape=()),\n",
    "    'num_features_nonzero': tf.placeholder(tf.int32)  # helper variable for sparse dropout\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = model_func(placeholders, input_dim=features[2][1], logging=True)\n",
    "\n",
    "# Initialize session\n",
    "sess = tf.Session()\n",
    "\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, support, labels, mask, placeholders):\n",
    "    t_test = time.time()\n",
    "    feed_dict_val = construct_feed_dict(features, support, labels, mask, placeholders)\n",
    "    outs_val = sess.run([model.loss, model.accuracy], feed_dict=feed_dict_val)\n",
    "    return outs_val[0], outs_val[1], (time.time() - t_test)\n",
    "\n",
    "\n",
    "# Init variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "cost_val = []\n",
    "\n",
    "# Train model\n",
    "for epoch in range(FLAGS.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    # Construct feed dictionary\n",
    "    feed_dict = construct_feed_dict(features, support, y_train, train_mask, placeholders)\n",
    "    feed_dict.update({placeholders['dropout']: FLAGS.dropout})\n",
    "\n",
    "    # Training step\n",
    "    outs = sess.run([model.opt_op, model.loss, model.accuracy], feed_dict=feed_dict)\n",
    "\n",
    "    # Validation\n",
    "    cost, acc, duration = evaluate(features, support, y_val, val_mask, placeholders)\n",
    "    cost_val.append(cost)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(outs[1]),\n",
    "          \"train_acc=\", \"{:.5f}\".format(outs[2]), \"val_loss=\", \"{:.5f}\".format(cost),\n",
    "          \"val_acc=\", \"{:.5f}\".format(acc), \"time=\", \"{:.5f}\".format(time.time() - t))\n",
    "\n",
    "    if epoch > FLAGS.early_stopping and cost_val[-1] > np.mean(cost_val[-(FLAGS.early_stopping+1):-1]):\n",
    "        print(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Testing\n",
    "test_cost, test_acc, test_duration = evaluate(features, support, y_test, test_mask, placeholders)\n",
    "print(\"Test set results:\", \"cost=\", \"{:.5f}\".format(test_cost),\n",
    "      \"accuracy=\", \"{:.5f}\".format(test_acc), \"time=\", \"{:.5f}\".format(test_duration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a7719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
